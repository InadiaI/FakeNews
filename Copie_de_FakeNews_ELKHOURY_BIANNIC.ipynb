{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copie de FakeNews-ELKHOURY-BIANNIC.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/InadiaI/FakeNews/blob/main/Copie_de_FakeNews_ELKHOURY_BIANNIC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sG6guxem5-8C"
      },
      "source": [
        "**Subject :** Create an algorithm for classifying news are \"fake\" or \"real\".\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "_______________________________________________________\n",
        "\n",
        "[1] Datasets from kaggle.com\n",
        "\n",
        "https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset\n",
        "\n",
        "https://www.kaggle.com/c/fake-news/data \n",
        "\n",
        "https://www.kaggle.com/hassanamin/textdb3\n",
        "\n",
        "https://www.kaggle.com/snapcrack/all-the-news (only \"real\")\n",
        "\n",
        "https://www.kaggle.com/mrisdal/fake-news (only fake)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87t38f2k61Nd"
      },
      "source": [
        "1- You need datasets containing news with a label \"fake\" or \"real\". You can use datasets from kaggle.com [1] or any other dataset you can find (github, google colab, ...).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZAuIa487TId",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "010bc952-5eac-4df2-f47c-34dcc12d8212"
      },
      "source": [
        "#We start by importing keras\n",
        "import keras\n",
        "keras.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.3'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ngpc59XCF1d"
      },
      "source": [
        "On importe la dataset de kaggle :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqGDiTvrCWZT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96124c35-c7e1-4cd1-b30d-4a6aad30a76d"
      },
      "source": [
        "!pip install kaggle\n",
        "import json\n",
        "import zipfile\n",
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"nadiaelkhoury\"\n",
        "os.environ['KAGGLE_KEY'] = \"0dda7b45f140e015af3441566c4be78d\"\n",
        "!kaggle datasets download -d clmentbisaillon/fake-and-real-news-dataset\n",
        "if not os.path.exists(\"/fakeNews/\"):\n",
        "    os.makedirs(\"/fakeNews\")\n",
        "os.chdir('/fakeNews')\n",
        "for file in os.listdir():\n",
        "  #zip = zipfile.ZipFile(file,'r')\n",
        "  #zip.extractall()\n",
        "  print(file)\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.9)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (0.0.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2020.6.20)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.10)\n",
            "fake-and-real-news-dataset.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "True.csv\n",
            "Fake.csv\n",
            "fake-and-real-news-dataset.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1ltR6CuCQ_u"
      },
      "source": [
        "On importe les différentes datasets True et Fake, on ajoute une colonne de labels pour pouvoir différencier les fake news des vrais ( 1 : true, à: fake) On va ensuite split le texte pour séparer chaque élément puis filtrer la ponctuation pour ne garder les mots.Enfin on concatène les deux bases de données en une seule qu'on appellera dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htcwQo66NVI8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0a03082-5801-4551-cbf1-7a87e0aff315"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "nltk.download('punkt')\n",
        "\n",
        "#import puis traitement et filtrage des données\n",
        "dataset1 = pd.read_csv('True.csv')\n",
        "dataset1[\"y\"] = 1\n",
        "for x in range(20):\n",
        "  split_text = word_tokenize(dataset1[\"text\"][x])\n",
        "  dataset1[\"text\"][x]= [word for word in split_text if word.isalpha()]\n",
        "\n",
        "dataset2 = pd.read_csv('Fake.csv')\n",
        "dataset2[\"y\"] = 0\n",
        "for x in range(20):\n",
        "  split_text = word_tokenize(dataset2[\"text\"][x])\n",
        "  dataset2[\"text\"][x]= [word for word in split_text if word.isalpha()]\n",
        "\n",
        "\n",
        "\n",
        "#concatenation\n",
        "#dataset = dataset1.append(dataset2)\n",
        "dataset = pd.concat((dataset1, dataset2),ignore_index= True)\n",
        "train,validate, test = np.split(dataset.sample(frac=1, random_state=42), [int(.6*len(dataset)), int(.8*len(dataset))])\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlwbrHJbBta1"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFH8D3G4NVdj"
      },
      "source": [
        "\n"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gq6pQIbPNjQU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyTLbmjF7M12"
      },
      "source": [
        "Analysis of the dataset. What kind of data is in your dataset ?\n",
        "Then you will process the data. Try to represent the data with Bag of Word\n",
        "Apply a standard (not deep learning) machine learning algorithm.\n",
        "Compute and analyze the performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Vi4r5ir7UH-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPpIeQT17Yr2"
      },
      "source": [
        "try to apply the same algorithm to a different dataset,\n",
        "use more complex representation of the data (Word2Vect, or anything else),\n",
        "try to improve or change the ML algorithm (for exemple try to use deepLearning),\n",
        "In each case, compare the performances obtained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpJPOzNZ7g8u"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7Z-L9ev7iNu"
      },
      "source": [
        "\n",
        "It is not expected that you try all of this, you are free to chose the representation of data that you want and the ML that you want. But be sure to understand what you are doing as you will need to present it.\n"
      ]
    }
  ]
}